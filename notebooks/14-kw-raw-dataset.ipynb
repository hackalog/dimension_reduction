{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets import build_dataset_dict, fetch_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a RawDataset estimator\n",
    "\n",
    "this has `fetch`, `unpack`, `process` methods.\n",
    "\n",
    "\n",
    "`fit`: fetch and hash-check the raw files\n",
    "\n",
    "`transform`: unpack and process the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = {\"broken-swiss-roll\": {\n",
    "        \"action\": \"generate\",\n",
    "        \"load_function_args\": [],\n",
    "        \"load_function_kwargs\": {\n",
    "            \"kind\": \"broken_swiss_roll\",\n",
    "            \"n_points\": 1000,\n",
    "            \"noise\": 0.05,\n",
    "            \"random_state\": 6502\n",
    "        },\n",
    "        \"load_function_module\": \"src.data.synthetic\",\n",
    "        \"load_function_name\": \"synthetic_data\",\n",
    "        \"rescale\": \"minmax\"\n",
    "    },\n",
    "}\n",
    "\n",
    "ds2 = {\n",
    "    \"f-mnist\": {\n",
    "        \"action\": \"fetch_and_process\",\n",
    "        \"load_function_args\": [],\n",
    "        \"load_function_kwargs\": {\n",
    "            \"dataset_name\": \"f-mnist\"\n",
    "        },\n",
    "        \"load_function_module\": \"src.data.localdata\",\n",
    "        \"load_function_name\": \"process_mnist\",\n",
    "        \"url_list\": [\n",
    "            {\n",
    "                \"hash_type\": \"md5\",\n",
    "                \"hash_value\": \"8d4fb7e6c68d591d4c3dfef9ec88bf0d\",\n",
    "                \"name\": \"training_data\",\n",
    "                \"url\": \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\"\n",
    "            },\n",
    "            {\n",
    "                \"hash_type\": \"md5\",\n",
    "                \"hash_value\": \"25c81989df183df01b3e8a0aad5dffbe\",\n",
    "                \"name\": \"training_labels\",\n",
    "                \"url\": \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\"\n",
    "            },\n",
    "            {\n",
    "                \"hash_type\": \"md5\",\n",
    "                \"hash_value\": \"bef4ecab320f06d8554ea6380940ec79\",\n",
    "                \"name\": \"test_data\",\n",
    "                \"url\": \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\"\n",
    "            },\n",
    "            {\n",
    "                \"hash_type\": \"md5\",\n",
    "                \"hash_value\": \"bb300cfdad3c16e7a12a480ee83cd310\",\n",
    "                \"name\": \"test_labels\",\n",
    "                \"url\": \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\"\n",
    "            }\n",
    "        ]\n",
    "    },    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from src.paths import (data_path, raw_data_path,\n",
    "                       interim_data_path, processed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource(BaseEstimator):\n",
    "    \"\"\"Representation of a raw dataset\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 name='raw_dataset',\n",
    "                 action=None,\n",
    "                 load_function=None,\n",
    "                 dataset_dir=None,\n",
    "                 file_list=None):\n",
    "        self.name = name\n",
    "        self.action = action\n",
    "        self.file_list = file_list\n",
    "        self.load_function = load_function\n",
    "        self.dataset_dir = dataset_dir\n",
    "    \n",
    "    def fit(self, X=None, y=None):\n",
    "        self.available_actions_ = ['generate', 'fetch']\n",
    "        if self.action not in self.available_actions_:\n",
    "               raise Exception(f'`action` must be one of {self.available_actions_}')\n",
    "        if self.file_list is None:\n",
    "            self.file_list = []\n",
    "        if self.dataset_dir is None:\n",
    "            self.dataset_dir = data_path\n",
    "            \n",
    "        self.fetched_ = False\n",
    "        self.unpacked_ = False\n",
    "        self.processed_ = False\n",
    "        self.fitted_ = True\n",
    "        \n",
    "    def fetch(self, fetch_path=None, force=False):\n",
    "        \"\"\"Fetch to raw_data_dir and check hashes\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'fitted_'):\n",
    "            raise Exception('must fit before feching')\n",
    "            \n",
    "        if self.fetched_ and force is False:\n",
    "            logger.debug(f'Raw Dataset {self.name} is already fetched. Skipping')\n",
    "            return\n",
    "        \n",
    "        if fetch_path is None:\n",
    "            fetch_path = raw_data_path\n",
    "        \n",
    "        for item in self.file_list:\n",
    "            pass\n",
    "            \n",
    "\n",
    "    def unpack(self, unpack_path=None, force=False):\n",
    "        \"\"\"Unpack fetched files to interim dir\"\"\"\n",
    "        if not hasattr(self, 'fitted_'):\n",
    "            raise Exception('must fit and fetch before unpack')\n",
    "        if not self.fetched_:\n",
    "            raise Exception(\"Must fetch before unpack\")\n",
    "            \n",
    "        if self.unpacked_ and force is False:\n",
    "            logger.debug(f'Raw Dataset {self.name} is already unpacked. Skipping')\n",
    "            return\n",
    "\n",
    "        if unpack_path is None:\n",
    "            unpack_path = interim_data_path\n",
    "\n",
    "    def process(self, processed_path=None):\n",
    "        if not hasattr(self, 'fitted_'):\n",
    "            raise Exception('must fit/fetch/unpack before process')\n",
    "        if not self.unpacked_:\n",
    "            raise Exception(\"Must fetch/unpack before process\")\n",
    "            \n",
    "        if self.processed_ and force is False:\n",
    "            logger.debug(f'Raw Dataset {self.name} is already processed. Skipping')\n",
    "            return\n",
    "        if processed_path is None:\n",
    "            processed_path = processed_data_path\n",
    "\n",
    "    def save(self, path=None, filename=\"datasets.json\", indent=4, sort_keys=True):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename=\"raw_dataset.json\", path=None):\n",
    "        \"\"\"Create a RawDataset from a (saved) json file.\n",
    "        \"\"\"\n",
    "        if path is None:\n",
    "            path = _MODULE_DIR\n",
    "        else:\n",
    "            path = pathlib.Path(path)\n",
    "\n",
    "        with open(path / filename, 'r') as fr:\n",
    "            ds = json.load(fr)\n",
    "\n",
    "        load_function = deserialize_partial(**ds)\n",
    "\n",
    "        return cls(**ds)\n",
    "    \n",
    "def deserialize_partial(func_dict):\n",
    "    \"\"\"Convert a serialized function call into a partial\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func_dict: dict containing\n",
    "        load_function_name: function name\n",
    "        load_function_module: module containing function\n",
    "        load_function_args: args to pass to function\n",
    "        load_function_kwargs: kwargs to pass to function\n",
    "    \"\"\"\n",
    "\n",
    "    args = func_dict.get(\"load_function_args\", [])\n",
    "    kwargs = func_dict.get(\"load_function_kwargs\", {})\n",
    "    base_name = func_dict.get(\"load_function_name\", 'unknown_function')\n",
    "    fail_func = partial(unknown_function, base_name)\n",
    "    func_mod_name = func_dict.get('load_function_module', None)\n",
    "    if func_mod_name:\n",
    "        func_mod = importlib.import_module(func_mod_name)\n",
    "    else:\n",
    "        func_mod = _MODULE\n",
    "    func_name = getattr(func_mod, base_name, fail_func)\n",
    "    func = partial(func_name, *args, **kwargs)\n",
    "    \n",
    "    return func\n",
    "    \n",
    "def serialize_partial(func):\n",
    "    \"\"\"Serialize a function call to a dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func: partial function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict containing:\n",
    "        load_function_name: function name\n",
    "        load_function_module: fully-qualified module name containing function\n",
    "        load_function_args: args to pass to function\n",
    "        load_function_kwargs: kwargs to pass to function\n",
    "    \"\"\"\n",
    "\n",
    "    func = partial(func)\n",
    "    entry = {}\n",
    "    entry['load_function_module'] = \".\".join(jfi.get_func_name(func.func)[0])\n",
    "    entry['load_function_name'] = jfi.get_func_name(func.func)[1]\n",
    "    entry['load_function_args'] = func.args\n",
    "    entry['load_function_kwargs'] = func.keywords\n",
    "    return entry\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = DataSource(action='fetch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dimension_reduction]",
   "language": "python",
   "name": "conda-env-dimension_reduction-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
